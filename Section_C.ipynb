{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RFE\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint, uniform\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#from collections import Counter\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Load and Explore Dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/Users/tusharjoshi/.pyenv/versions/3.12.7/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "\n",
    "# Step 2: Load and Explore Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    return data \n",
    "   \n",
    "\n",
    "def preprocess_and_split_data(data, target_column):\n",
    "    \n",
    "    ''' This is another way to implement label encoder\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for column_name in data.columns:\n",
    "       if data[column_name].dtype == object:\n",
    "          data[column_name] = le.fit_transform(data[column_name])\n",
    "     else:\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "    X = data.drop(columns=[target_column])  # Drop the target column to get the features\n",
    "    y = data[target_column]  # Extract the target variable\n",
    "\n",
    "     # Feature Engineering\n",
    "    X['balance_salary_ratio'] = X['balance'] / (X['estimated_salary'] + 1)\n",
    "    X['age_group'] = pd.cut(X['age'], bins=[0, 30, 45, 60, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for column in X.select_dtypes(include=['object']):  # For object columns only\n",
    "        X[column] = le.fit_transform(X[column])\n",
    "    \n",
    "    # Encode the target variable if it's not already numeric\n",
    "    if y.dtype == 'object':\n",
    "        y = le.fit_transform(y)\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    #Convert float columns to integers\n",
    "    #for column in X.select_dtypes(include=['float']):  # For float columns only\n",
    "    #    X[column] = X[column].astype(int)  # Convert to integer\n",
    "   \n",
    "    # Feature Selection\n",
    "    rfe = RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=15), n_features_to_select=8)\n",
    "    X = pd.DataFrame(rfe.fit_transform(X, y), columns=X.columns[rfe.support_])\n",
    "\n",
    "    # Handle Class Imbalance\n",
    "    smote = SMOTE(random_state=15)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=15, stratify=y_resampled)  # Split the data into training and  testing sets,and use stratify to approximately maintain the imbalanced ratio of taget column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Step 3: Train Random Forest Model\n",
    "def train_random_forest(X_train, y_train):\n",
    "    \n",
    "    #print(\"counter before SMOTE:\", Counter(y_train))\n",
    "    #Apply SMOTE for resampling\n",
    "    #smote = SMOTE(random_state=15)\n",
    "    #X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    #print(\"Counter after SMOTE:\", Counter(y_resampled))\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=15, class_weight='balanced')  # Create an instance of RandomForestClassifier with fixed random state\n",
    "    rf.fit(X_train, y_train)  # Fit the model to the training data\n",
    "    print(\"these are the params\",rf.get_params())\n",
    "    return rf  # Return the trained model\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Step 3: Train Random Forest Model with Hyperparameter Tuning\n",
    "def train_random_forest_with_tuning(X_train, y_train):\n",
    "    # Define hyperparameter space for Random Search\n",
    "    param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees in the forest\n",
    "    'max_depth': [None] + list(range(5, 20)),  # Depth of each tree\n",
    "    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 10),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    'criterion': ['gini', 'entropy']  # Function to measure the quality of a split\n",
    "    }\n",
    "    \n",
    "    # Create RandomizedSearchCV object and fit it to training data\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=15, class_weight='balanced')\n",
    "    randomized_search = RandomizedSearchCV(estimator=rf,\n",
    "                                        param_distributions=param_dist,\n",
    "                                        n_iter=100,  # Number of parameter settings sampled\n",
    "                                        cv=5,  # Cross-validation splitting strategy\n",
    "                                        scoring='accuracy',  # Metric to optimize\n",
    "                                        n_jobs=-1,  # Use all available cores\n",
    "                                        random_state=15)\n",
    "    \n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    # Return the best model found by RandomizedSearchCV\n",
    "    return randomized_search.best_estimator_, randomized_search.best_params_, randomized_search.best_score_\n",
    "\n",
    "'''\n",
    "\n",
    "# Step 3: Train XGBoost Model with Hyperparameter Tuning\n",
    "def train_xgboost_with_tuning(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4)\n",
    "    }\n",
    "\n",
    "    xgb = XGBClassifier(random_state=15, use_label_encoder=False, eval_metric='logloss')\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    \n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=15\n",
    "    )\n",
    "    \n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    return randomized_search.best_estimator_, randomized_search.best_params_, randomized_search.best_score_\n",
    "\n",
    "# Step 4: Evaluate Model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)  # Get the model's predictions on the test data\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate F1 score (weighted average for imbalanced classes)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()  # Get the confusion matrix\n",
    "    specificity = tn / (tn + fp)  # Calculate specificity\n",
    "\n",
    "    # Display metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score (Weighted):\", f1)\n",
    "    print(\"Precision (Weighted):\", precision)\n",
    "    print(\"Recall (Weighted):\", recall)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))  # Detailed classification report\n",
    "\n",
    "\n",
    "\n",
    "#print(\"\\nResults with SMOTE and class_weight='balanced':\")\n",
    "#print(classification_report(y_test, rf_pred))\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Main Workflow\n",
    "def run_analysis(file_path, target_column):\n",
    "    # Load data\n",
    "    data = load_data(file_path)\n",
    "\n",
    "    #this is part of EDA, sort it out later\n",
    "    print(\"Data type:\", type(data))\n",
    "    print(\"Data columns:\", data.columns)\n",
    "    print(\"First few rows of data:\\n\", data.head())\n",
    "    print(\"Null values in each column:\\n\", data.isnull().sum())\n",
    "\n",
    "    #process and split data\n",
    "    X_train, X_test, y_train, y_test = preprocess_and_split_data(data, target_column)\n",
    "    \n",
    "    # EDA as well\n",
    "    print(\"Shape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "\n",
    "     # Train the Random Forest model with hyperparameter tuning\n",
    "    best_model, best_params, best_score = train_xgboost_with_tuning(X_train, y_train)\n",
    "\n",
    "    # Evaluate the best model on test data\n",
    "    print(f\"\\nEvaluation Metrics for {file_path} using randomsearchcv:\\n\")\n",
    "    evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "    # Print best parameters and score from tuning\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    print(\"Best cross-validated score:\", best_score)\n",
    "\n",
    "    '''\n",
    "    # Train the Random Forest model\n",
    "    model = train_random_forest(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model and print metrics\n",
    "    print(f\"\\nEvaluation Metrics for {file_path}:\\n\")\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    '''\n",
    "\n",
    "\n",
    "# Step 6: Test with different datasets\n",
    "file_path = './BankCustomerChurnPrediction.csv'  # Replace with actual file path of Dataset 1\n",
    "target_column = 'churn'  # Replace with the actual target column name\n",
    "run_analysis(file_path, target_column)\n",
    "\n",
    "\n",
    "#this is part of EDA\n",
    "#data = pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Architecture: 64-bit\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "print(f\"Python Architecture: {struct.calcsize('P') * 8}-bit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_data(file_path)\n",
    "data.info()\n",
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :\\n\\n', categorical)\n",
    "print(data[categorical].head())\n",
    "\n",
    "numerical = [var for var in data.columns if data[var].dtype!='O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The numerical variables are :\\n\\n', numerical)\n",
    "data[numerical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
